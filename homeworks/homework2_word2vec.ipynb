{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m97Nru47E6k9"
      },
      "outputs": [],
      "source": [
        "# python standard library imports\n",
        "# from-imports follow regular imports\n",
        "import re\n",
        "import random\n",
        "from math import sqrt\n",
        "from collections import Counter\n",
        "\n",
        "# external libraries imports, grouped semantically\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from IPython.display import clear_output, display"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2vec tutorial\n",
        "\n",
        "In this tutorial we will implement text pre-processing in a matte that is similar to how we pre-process text for neural networks and then we will train out own word vectors on our own dataset.\n",
        "\n",
        "## Task 1\n",
        "Find a dataset to train your word embeddings on. It should be more than 50 000 words after the pre-processing is done. To make the dataset you can use absolutely any text: wikipedia, movie scripts, books, reddit, twitter. The kinds of text you are using will affect your word embeddings flavor. You can use the same dataset we used in the class, but we encourage you to make your own dataset.\n",
        "\n",
        "Useful links:\n",
        "\n",
        "1. Learn more about python list comprehensions: https://www.programiz.com/python-programming/list-comprehension\n",
        "1. Numerical instability of softmax: https://www.youtube.com/watch?v=99qLd2p6x5E"
      ],
      "metadata": {
        "id": "082pyht2Y9V_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"star_wars_scripts.txt\") as f:\n",
        "    raw_data = f.read()"
      ],
      "metadata": {
        "id": "JJ1cBurDE-Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_data[:1000])"
      ],
      "metadata": {
        "id": "8ldnFGh8G7ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data[:1000]"
      ],
      "metadata": {
        "id": "8MfgDBkCG9GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "X2ZP75Y7IbwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "swUIl9RLHD8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_data = preprocess(raw_data)\n",
        "preprocessed_data[:100]"
      ],
      "metadata": {
        "id": "nF3mpXIsHjvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    return text.split(\" \")\n",
        "\n",
        "tokenized_data = tokenize(preprocessed_data)\n",
        "\n",
        "print(f\"Total number of words : {len(tokenized_data)}\")\n",
        "print(f\"Number of unique words: {len(set(tokenized_data))}\")\n",
        "print(f\"First 10 words\", tokenized_data[:10])"
      ],
      "metadata": {
        "id": "tt1ZW2oOHpwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = Counter(tokenized_data)\n",
        "word_counts.most_common(10)"
      ],
      "metadata": {
        "id": "x8B8Y021IKDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# least common\n",
        "word_counts.most_common()[:-10-1:-1]"
      ],
      "metadata": {
        "id": "Of-oP7UwK21p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rare_words = []\n",
        "for word, count in word_counts.items():\n",
        "    if count < 5:\n",
        "        rare_words.append(word)\n",
        "\n",
        "len(rare_words)"
      ],
      "metadata": {
        "id": "V0vJI6vGIv3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After looking at the resulting word vectors I decided to remove some of the most frequent words too. While in general it is not a good idea, it words well for this homework."
      ],
      "metadata": {
        "id": "Vd8PNRGii6rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frequent_words = [\"the\", \"and\", \"a\", \"to\", \"of\", \"in\", \"his\", \"you\", \"is\", \"on\", \"i\", \"as\", \"he\", \"it\", \"at\", \"with\", \"into\", \"out\", \"him\", \"from\", \"for\", \"that\", \"be\", \"your\", \"by\", \"this\", \"an\"]"
      ],
      "metadata": {
        "id": "6ivO_fpAimIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = set(tokenized_data) - set(rare_words) - set(frequent_words)\n",
        "len(vocabulary)"
      ],
      "metadata": {
        "id": "q8BGDi_mKoJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data = [w for w in tokenized_data if w in vocabulary]\n",
        "\n",
        "print(\"Total number of words in the dataset after filtration: \", len(filtered_data))\n",
        "assert len(filtered_data) > 50_000, \"your dataset should have at least 50K words\""
      ],
      "metadata": {
        "id": "7ZmBKRlBOI66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2vec idea:\n",
        "\n",
        "Maximize in-context word similarity, while minimizing out-of-context word vector similarity.\n",
        "\n",
        "Spefcifically:\n",
        "\n",
        "$$\n",
        "maximize \\frac{e^{w \\cdot w_{in context}}}{\\sum e^{ w \\cdot w_{out of context}}}\n",
        "$$"
      ],
      "metadata": {
        "id": "ngqaZ-s2LirI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 4"
      ],
      "metadata": {
        "id": "Qc5Axu7xN5SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data[110:130]\n",
        "# word: spacecraft, context: [tiny, silver, a, rebel]"
      ],
      "metadata": {
        "id": "HQFghpwsNbGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_and_contexts = []  # list of tuples (word, list of context words)\n",
        "\n",
        "for i, word in enumerate(filtered_data):\n",
        "    if i < window_size:\n",
        "        continue\n",
        "    if i >= len(filtered_data) - window_size:\n",
        "        break\n",
        "\n",
        "    context = []\n",
        "    for j in range(i - window_size, i + window_size + 1):\n",
        "        if i == j: continue\n",
        "        context.append(filtered_data[j])\n",
        "    \n",
        "    word_and_context = (word, context)\n",
        "    words_and_contexts.append(word_and_context)\n",
        "\n",
        "random.shuffle(words_and_contexts)\n",
        "words_and_contexts[0]"
      ],
      "metadata": {
        "id": "48Lt8zgDN3QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# (Too) simple (and too slow) word2vec implementation\n",
        "\n",
        "```python\n",
        "emb_size = 50\n",
        "\n",
        "word2vec = dict()\n",
        "for w in vocabulary:\n",
        "    word2vec[w] = torch.randn(emb_size, requires_grad=True)\n",
        "    \n",
        "# training loop\n",
        "\n",
        "for word, context in tqdm(words_and_contexts):\n",
        "    word_vector = word2vec[word]\n",
        "    context_vector = sum([word2vec[w] for w in context]) / len(context)\n",
        "\n",
        "    nominator = torch.exp(word_vector @ context_vector.T)\n",
        "    denominator = sum(torch.exp(word_vector @ word2vec[w].T) for w in vocabulary)\n",
        "\n",
        "    loss = - nominator / denominator\n",
        "\n",
        "    # update our word vectors using gradient descent to minimize loss\n",
        "    # 1. compute gradients\n",
        "    # 2. update vectors in the direction opposite to the gradient\n",
        "\n",
        "    loss.backward()  # compute gadients\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for w in word2vec.values():\n",
        "            w -= w.grad\n",
        "            w.grad = None\n",
        "\n",
        "# TOO SLOW: one pass over data takes more than an hour!\n",
        "```"
      ],
      "metadata": {
        "id": "ToxvCTXGP1AD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More realistic word2vec example\n",
        "\n",
        "We going to change a couple of things from the naive implementation:\n",
        "\n",
        "1. Use a single embedding matrix instead of a dicrionary `word2vec` that maps words to their vectors\n",
        "2. Use matrix multiplication instead of for-loop\n",
        "\n",
        "These two things will speed up computation by a factor of a hundred.\n",
        "\n",
        "3. Use a numerically stable implementation of the loss function.\n",
        "\n",
        "4. Update our vectors with a gradient averaged over 100 examples instead of a gradient of just one example.\n",
        "\n",
        "5. Initialize word vectors with smaller numbers.\n",
        "\n",
        "6. Carefully select **learning rate** parameter of gradient descent.\n",
        "\n",
        "These four things (3-6) will help us to optimize word vectors without them becoming NaN.\n",
        "\n",
        "7. We are going to train for longer to get better vectors\n",
        "\n",
        "### About numerical stability\n",
        "\n",
        "Remember that our loss looks like this:\n",
        "\n",
        "$$\n",
        "loss = -log \\frac{e^{w \\cdot c}}{\\sum e^{w \\cdot w_i}}\n",
        "$$\n",
        "\n",
        "which is basically a softmax operation\n",
        "\n",
        "$$\n",
        "softmax(x) = \\frac{e^{x}}{\\sum e^{x_i}}\n",
        "$$\n",
        "\n",
        "And naive implementation of softmax causes NaN when computed on large numbers. Watch [the video](https://youtu.be/99qLd2p6x5E?t=92) to understand why.\n",
        "\n",
        "If you prefer reading to watching the videos, you can take a look at this [Stanford CS231n class note](https://cs231n.github.io/linear-classify/#softmax-classifier). Look for \"Practical issues: Numeric stability\".\n",
        "\n",
        "### About word vector initialization\n",
        "\n",
        "In the next lectures we will discuss why it's beneficial to initialize parameters like randn(size) / sqrt(size).\n",
        "Short answer: large numbers cause divergences in optimization process. Meaning you loss goes to infinity or NaN which is just as bad. When you multiply two random vectors of size S distributed N(0, 1) you get a vector distribted N(0, S). To make this smaller we initialize vectors from $N(0, 1/S) = N(0, 1) / \\sqrt{S}$ which gives us N(0, 1).\n",
        "\n"
      ],
      "metadata": {
        "id": "trEEC9wgduls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2id = {}\n",
        "for idx, word in enumerate(vocabulary):\n",
        "    word2id[word] = idx\n",
        "\n",
        "\n",
        "def numerically_stable_loss(word_vector, context_vector, all_word_vectors):\n",
        "    word_context_sim = word_vector @ context_vector.T\n",
        "    word_vocab_sim = word_vector @ all_word_vectors.T\n",
        "\n",
        "    # --- important part that makes computation numerically stable ---\n",
        "    max_value = torch.max(word_vocab_sim)\n",
        "    word_context_sim_normalized = word_context_sim - max_value\n",
        "    word_vocab_sim_normalized = word_vocab_sim - max_value\n",
        "    # ----------------------------------------------------------------\n",
        "\n",
        "    nominator = torch.exp(word_context_sim_normalized)\n",
        "    denominator = torch.sum(torch.exp(word_vocab_sim_normalized))\n",
        "\n",
        "    loss = -torch.log(nominator / denominator)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "wozAr1sVj_H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2:\n",
        "\n",
        "This training loop is already working, your task is to understand how it works and to find `learning_rate` that gives you the best vectors in terms of the training loss. Also feel free to modify `emb_size`, `max_epochs` and `batch_size`, but usually learning rate is the most important parameter. Reasonable range for learning rate (in this specific case) is 1e-4 to 1.0, but feel free to try values outsize of that range as well."
      ],
      "metadata": {
        "id": "-dKfNVBPcrIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_size = 50\n",
        "learning_rate = 1e-3\n",
        "\n",
        "vocab_size = len(vocabulary)\n",
        "batch_size = 100\n",
        "\n",
        "word_embeddings_init_value = np.random.randn(vocab_size, emb_size) / sqrt(emb_size)\n",
        "word_embeddings = torch.tensor(word_embeddings_init_value, requires_grad=True, dtype=torch.float32)\n",
        "\n",
        "global_step = 0\n",
        "\n",
        "prev_loss_value = None\n",
        "loss_hist = []\n",
        "should_stop = False\n",
        "\n",
        "for epoch in range(5):\n",
        "    random.shuffle(words_and_contexts)  # shuffle the dataset\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    if should_stop: break\n",
        "\n",
        "    for word, context in tqdm(words_and_contexts):\n",
        "        global_step += 1\n",
        "\n",
        "        word_idx = word2id[word]\n",
        "        context_idx = [word2id[w] for w in context]\n",
        "\n",
        "        word_vector = word_embeddings[word_idx]\n",
        "        context_vector = sum(word_embeddings[context_idx])\n",
        "\n",
        "        loss = numerically_stable_loss(word_vector, context_vector, word_embeddings)\n",
        "\n",
        "        # compute gadients at every step\n",
        "        # every loss.backward() adds to the previous gradients\n",
        "        # using multiple examples to get the gradient allows us to better estimate the true gradient over the whole dataset\n",
        "        loss.backward()\n",
        "\n",
        "        if torch.isnan(loss).item():\n",
        "            print(\"Loss is None. Updating the vectors now will cause them to become NaN. Interrupting training.\")\n",
        "            should_stop = True\n",
        "            break\n",
        "\n",
        "        if global_step % batch_size == 0:\n",
        "            with torch.no_grad():\n",
        "                if torch.any(torch.isnan(word_embeddings.grad)):\n",
        "                    print(\"Grad is None. Updating the vectors now will cause them to become NaN. Interrupting training.\")\n",
        "                    should_stop = True\n",
        "                    break\n",
        "\n",
        "                gradient = word_embeddings.grad\n",
        "                gradient = gradient / batch_size  # average gradients over batch_size examples\n",
        "                gradient = gradient.clamp(min=-5.0, max=5.0)  # fancy deep learning hack that we will learn in the next lectures\n",
        "                word_embeddings -= learning_rate * gradient\n",
        "\n",
        "                # zero out the gradients\n",
        "                word_embeddings.grad = torch.zeros_like(word_embeddings.grad)\n",
        "\n",
        "        if prev_loss_value is None:\n",
        "            prev_loss_value = loss\n",
        "\n",
        "        loss_to_log = loss * 0.1 + prev_loss_value * 0.9  # exponential smoohting to make the plots look nicer\n",
        "        prev_loss_value = loss_to_log\n",
        "        loss_hist.append(loss_to_log.item())  # .item() converts a single-value tensor to a regular python float\n",
        "\n",
        "        if global_step % 10_000 == 0:\n",
        "            # plots the loss so you can observe if it goes down or not\n",
        "            clear_output()  # this will cause tqdm to disappear, sorry\n",
        "            fig, ax = plt.subplots(1, 1)\n",
        "            ax.plot(loss_hist)\n",
        "            display(fig)\n",
        "\n",
        "print(\"Training finished\")"
      ],
      "metadata": {
        "id": "UPPMAF0yevH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**More about task 2** We expect your loss to significantly decrease during training which should be visible on the plot above."
      ],
      "metadata": {
        "id": "fHYzG4AbgW7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Play with trained vectors\n",
        "\n",
        "def get_word_vector(word_str):\n",
        "    if word_str not in word2id:\n",
        "        raise ValueError(f\"Word {word_str} not in the vocabulary\")\n",
        "    \n",
        "    return word_embeddings[word2id[word_str]]"
      ],
      "metadata": {
        "id": "lFvKShyiqpWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "luke = get_word_vector(\"luke\")\n",
        "force = get_word_vector(\"force\")\n",
        "spaceship = get_word_vector(\"spaceship\")\n",
        "vader = get_word_vector(\"vader\")"
      ],
      "metadata": {
        "id": "YxREa-TrrOuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute similarity of the word luke with the word force\n",
        "luke @ force.T"
      ],
      "metadata": {
        "id": "mZAPY0SFrU-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "luke @ spaceship.T"
      ],
      "metadata": {
        "id": "RDOiGI5YrWWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "luke @ vader.T"
      ],
      "metadata": {
        "id": "2pjI9xTcrpLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3:\n",
        "\n",
        "Use word embeddings to find most similar words to these words:\n",
        "\n",
        "1. luke\n",
        "1. woman\n",
        "1. star\n",
        "\n",
        "If the above words are not in your vocabulary, feel free to use different ones.\n",
        "\n",
        "\n",
        "> Because the dataset is small, it's likely that most similar words won't be perfect. Feel free to train on more data (get a bigger dataset).\n",
        "\n",
        "\n",
        "To find most similar words to a word W compute the similarity with the rest of the words from the vocabulary. Print top-10 most similar words: most similar to least and the similarity value like this:\n",
        "\n",
        "```\n",
        "luke most similar:\n",
        "luke similarity: 18.753528594970703\n",
        "han similarity: 17.64794921875\n",
        "leia similarity: 16.81419563293457\n",
        "threepio similarity: 14.953056335449219\n",
        "int similarity: 14.128015518188477\n",
        "cockpit similarity: 12.512998580932617\n",
        "star similarity: 11.790962219238281\n",
        "artoo similarity: 11.107627868652344\n",
        "chewie similarity: 10.742396354675293\n",
        "death similarity: 10.645984649658203\n",
        "```"
      ],
      "metadata": {
        "id": "2M3vD31lfIi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n"
      ],
      "metadata": {
        "id": "Ew7DsYiarZv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission instructions\n",
        "\n",
        "As in the first homework, execute the notebook top-to-bottom and submit it to the blackboard."
      ],
      "metadata": {
        "id": "a1x1iQEflYD9"
      }
    }
  ]
}